{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee88e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numba import jit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cab92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_file = \"word/dict.txt\"\n",
    "stop_file = \"word/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d21d7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cut(text):\n",
    "    jieba.load_userdict(dic_file) # 可以在jieba词典中添加新词\n",
    "    jieba.initialize()\n",
    "    try:\n",
    "        stopword_list = open(stop_file,encoding ='utf-8')\n",
    "    except:\n",
    "        stopword_list = []\n",
    "        print(\"error in stop_file\")\n",
    "    stop_list = []\n",
    "    flag_list = ['n','nz','vn'] # 设定只需要名词、专有名词、动名词\n",
    "    # 读取设定的停用词\n",
    "    for line in stopword_list:\n",
    "        line = re.sub(u'\\n|\\\\r', '', line)\n",
    "        stop_list.append(line)\n",
    "    \n",
    "    word_list = []\n",
    "    #分词\n",
    "    seg_list = psg.cut(text)\n",
    "    for seg_word in seg_list:\n",
    "        word = re.sub(u'[^\\u4e00-\\u9fa5]','',seg_word.word)\n",
    "        find = 0\n",
    "        # 不记录停用词以及小于两个字的词\n",
    "        if word in stop_list or len(word) < 2:\n",
    "            continue\n",
    "        if seg_word.flag in flag_list:\n",
    "            word_list.append(word) \n",
    "    return word_list\n",
    "\n",
    "def text_handling(data):\n",
    "    freq = {}\n",
    "    word_list=[]\n",
    "    for i in range(len(data)):\n",
    "        word_list.append([])\n",
    "        word_list[-1] = word_cut(data[i])\n",
    "        for word in word_list[i]:\n",
    "            if word in freq.keys():\n",
    "                freq[word] += 1\n",
    "            else:\n",
    "                freq[word] = 1\n",
    "    return word_list, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69ee0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDATopicModel:\n",
    "    \n",
    "    def __init__(self, data, topic_num, alpha, beta):\n",
    "        self.data = data\n",
    "        self.text_num = len(data)\n",
    "        self.topic_num = topic_num\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.word_map = None\n",
    "        self.word_list = None\n",
    "        self.word_idx = None\n",
    "\n",
    "\n",
    "    def preprocess(self, freq, drop_num):\n",
    "        all_words = []\n",
    "        \n",
    "        for i in range(self.text_num):\n",
    "            all_words = all_words + self.data[i]\n",
    "            \n",
    "        unique_words = list(set(all_words))\n",
    "        unique_words.sort(key=lambda wd: freq[wd], reverse=True)\n",
    "        self.word_list = unique_words[drop_num:]\n",
    "        \n",
    "        self.word_idx = {}\n",
    "        for t, wd in enumerate(self.word_list):\n",
    "            self.word_idx[wd] = t\n",
    "            \n",
    "        self.word_map = []\n",
    "        for word_line in self.data:\n",
    "            self.word_map.append([])\n",
    "            for word in word_line:\n",
    "                if word in self.word_list:\n",
    "                    self.word_map[-1].append(self.word_idx[word])\n",
    "\n",
    "    def fit(self, epochs):\n",
    "\n",
    "        word_num = len(self.word_list)\n",
    "        # 设置超参\n",
    "        alpha = self.alpha * np.ones(self.topic_num)\n",
    "        beta = self.beta * np.ones(word_num)\n",
    "        # 初始化参数\n",
    "        nd = np.zeros((self.text_num, self.topic_num)).astype(np.int64)\n",
    "        mk = np.zeros((self.topic_num, word_num)).astype(np.int64)\n",
    "        z  = [np.random.randint(0, self.topic_num, len(self.word_map[i])) for i in range(0, self.text_num)]\n",
    "        new_z = [np.zeros(len(self.word_map[i])).astype(np.int64) for i in range(0, self.text_num)]\n",
    "        # 统计变量\n",
    "        for i, wordlist in enumerate(self.word_map):\n",
    "            mk,nd = self.Gibbs_0(np.array(wordlist),mk,nd,i,np.array(z[i]))\n",
    "\n",
    "        # 吉布斯采样\n",
    "        for epoch in range(1, epochs+1):\n",
    "            if epoch % 5 == 0:\n",
    "                print('--第',epoch,'次迭代--')\n",
    "            \n",
    "            temp = (np.sum(mk, axis=1) + np.sum(beta))*(np.sum(nd) + np.sum(alpha))\n",
    "            for i, wordlist in enumerate(self.word_map):\n",
    "                new_z[i] = list(self.Gibbs_1(np.array(wordlist),temp,mk,beta,alpha,nd,i,np.array(new_z[i])))\n",
    "                \n",
    "            for i, wordlist in enumerate(self.word_map):\n",
    "                mk,nd,z[i] = self.Gibbs_2(np.array(wordlist),mk,nd,np.array(z[i]),i,np.array(new_z[i]))\n",
    "\n",
    "        # 结果\n",
    "        self.phi = mk + np.broadcast_to(beta, (self.topic_num, word_num))\n",
    "        self.phi = self.phi / np.transpose(np.broadcast_to(np.sum(self.phi, axis=1), (word_num, self.topic_num)))\n",
    "\n",
    "        self.theta = nd + np.broadcast_to(alpha, (self.text_num, self.topic_num))\n",
    "        self.theta = self.theta / np.transpose(np.broadcast_to(np.sum(self.theta, axis=1), (self.topic_num, self.text_num)))\n",
    "\n",
    "            \n",
    "    def topics_words(self, word_num):\n",
    "        # 返回每个主题的前n个词语\n",
    "        topic_word = np.argsort(-self.phi, axis=1)\n",
    "        top_words = [[]] * self.topic_num\n",
    "        for i in range(self.topic_num):\n",
    "            top_words[i] = []\n",
    "            for j in range(word_num):\n",
    "                top_words[i].append(self.word_list[topic_word[i][j]])\n",
    "        return top_words\n",
    "\n",
    "\n",
    "    def text_topics(self, n):\n",
    "        return np.argsort(-self.theta, axis=1)[:,:n]\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit(nopython=True)\n",
    "    def Gibbs_0(wordlist,mk,nd,i,z):\n",
    "        for j, word in enumerate(wordlist):\n",
    "            nd[i, z[j]] += 1\n",
    "            mk[z[j], word] += 1\n",
    "        return mk, nd\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit(nopython=True)\n",
    "    def Gibbs_1(wordlist,temp,mk,beta,alpha,nd,i,new_z):\n",
    "        for j, word in enumerate(wordlist):\n",
    "            p = (mk[:,word]+beta[word])*(nd[i,:]+alpha) / temp\n",
    "            p = p / np.sum(p)\n",
    "            if not (p > 0.).all():\n",
    "                print(p)\n",
    "            new_z[j] = np.argmax(np.random.multinomial(1, p))\n",
    "        return new_z\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit(nopython=True)\n",
    "    def Gibbs_2(wordlist, mk, nd, z, i, new_z):\n",
    "        for j, word in enumerate(wordlist):\n",
    "            mk[z[j], word] -= 1\n",
    "            mk[new_z[j], word] += 1\n",
    "            nd[i, z[j]] -= 1\n",
    "            nd[i, new_z[j]] += 1\n",
    "            z[j] = new_z[j]\n",
    "        return mk, nd, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5ec053",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "368b5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, freq = text_handling(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f8b8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--第 5 次迭代--\n",
      "--第 10 次迭代--\n",
      "--第 15 次迭代--\n",
      "--第 20 次迭代--\n",
      "--第 25 次迭代--\n",
      "--第 30 次迭代--\n",
      "--第 35 次迭代--\n",
      "--第 40 次迭代--\n",
      "--第 45 次迭代--\n",
      "--第 50 次迭代--\n",
      "学习结束，预处理用时2.664484977722168s, 迭代用时3.153074026107788s。\n"
     ]
    }
   ],
   "source": [
    "lda = LDATopicModel(data=word_list,topic_num=8,alpha=0.3,beta=0.3)\n",
    "start = time.time()\n",
    "lda.preprocess(freq=freq,drop_num=0)\n",
    "end1 = time.time()\n",
    "lda.fit(epochs=50)\n",
    "result = lda.topics_words(word_num=15)\n",
    "end2 = time.time()\n",
    "print(\"学习结束，预处理用时{}s, 迭代用时{}s。\".format(end1-start,end2-end1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96ad0000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : ['游戏', '票房', '玩家', '公司', '电子竞技', '研究', '世界', '科学家', '任务', '航天飞机', '网游', '太空', '补偿', '技术', '消息']\n",
      "Topic 2 : ['专家', '走势', '分析', '突破', '黄金', '股票', '大盘', '成本', '公司', '市场', '趋势', '股市', '风险', '后市', '投资']\n",
      "Topic 3 : ['主持人', '电影', '比赛', '现场', '电子竞技', '总决赛', '观众', '演员', '项目', '战队', '赛区', '赛事', '冠军', '游戏', '影片']\n",
      "Topic 4 : ['手机', '网站', '网游', '用户', '互联网', '政策', '电影', '公司', '网络', '视频', '预测', '市场', '业务', '文章', '票房']\n",
      "Topic 5 : ['主队', '赔率', '市场', '海选', '公司', '客胜', '投资', '项目', '政策', '客队', '流感', '交通', '足彩', '博彩', '主场']\n",
      "Topic 6 : ['比赛', '球队', '主场', '火箭', '球员', '客场', '篮板', '奇才', '联赛', '体育讯', '助攻', '本场', '湖人', '训练', '内线']\n",
      "Topic 7 : ['项目', '建筑', '地产', '空间', '产品', '投资', '设计', '公司', '文化', '别墅', '市场', '国际', '房子', '主持人', '企业']\n",
      "Topic 8 : ['学生', '大学', '电影', '学校', '专业', '国家', '移民', '教育', '孩子', '国际', '留学生', '研究', '记者', '影片', '银行']\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(\"Topic\",i+1,\":\",result[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
